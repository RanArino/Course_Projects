Reflection Paper

Name: Ran Arino
ID: 153073200
 

1. What data structure did you use to write this project? Why? 

I mainly used four structures that we learned in this class; list, set, dictionary, and binary tree. I used the binary tree to store every character of each word. The purpose is to retrieve the alphabetical order of each word by running the in-order tree traversal. The outcome of this traversal (the list of a word's characters in alphabetical order) will function as a unique dictionary key. In other words, more than two anagram words can have the same dictionary key. The reason why I used a dictionary is to improve the accessibility during the searching phase. Once the algorithm runs the in-order traversal from a created binary tree based on the word of user input, we can obtain a dictionary key, which is a list of word characters ordered alphabetically. Also, when we search for whether a certain item exists in a dictionary, we don't have to traverse every item, which means this operation completes by constant time, rather than O(n), where n is the number of a dictionary key. For instance, if the outcome of "dictionary.get(key)" is None, its key doesn't exist. Furthermore, the dictionary key enables users to access its value with constant time as well. In my codes, each value of the dictionary ("freq_dict") corresponding to its key has a list of two items; a set of unique anagram words and the frequency of their words. The first item is a set of unique words. For example, if the algorithm finds the words "mean" and "name" whose alphabetical order is the same, those two words will be added to this set. Even if the same word is added to this set, the uniqueness of words will be maintained by the feature of the set. This is one of the useful points of the set structure. The second item is the frequency of words, which is the first item of each anagram dictionary. Whenever the algorithm finds a word in a set of anagram words, the count will be incremented. 

   

2. What is the time complexity of your code for part 1? How can you improve it? 

The best case is O(n1), where n1 is the number of characters. It will happen when a given character is not an alphabet, a character is the first item, and the previous character of a character is not an alphabet. However, the worst case is O(n1*179*n3), where n1 is the number of characters in the text file by a for loop, the number “179” shows the maximum times to research for whether each word is in the list "stop_words", and n3 is the number of characters of each word by an in-order traverse. 

There were two main considerations about time complexity in my function “anagram_finder”: 

(1): No use of re-module (Regular expression operations) and gensim 
I tried to use re module in order to remove punctuations, digits and stop words by the following code: 
 
import re 
from gensim.parsing.preprocessing import remove_stopwords 
with open('adventures_of_huckleberry_finn.txt', 'r') as f: 
    data = f.read() 
    # \W: punctuations or \d: decimal digit 
    pattern = re.compile(r"\W|\d") 
    # if a word matches "pattern" replaces the blank ' ' 
    data = remove_stopwords(pattern.sub(' ', data.lower())) 
 
However, I realized that this process traverses the whole words twice by “remove_stopwords” and “pattern.sub()”. It means that the time complexity would be more than O(n**2) at the phase of importing text file (never touching the process to find anagram). 

 

(2): The short searching by a dictionary structure (line 41) 
I used “get()” method in line 41, so the computer no longer needs to traverse items to check if a given item exists in the collections, which is run by the "in" operator of a list or set structure. In other words, if “dictionary.get(key)” returns “None”, it means that this “key” doesn’t exist in a dictionary. Therefore, I succeeded in decreasing time complexity to O(1) from O(n) in the worst case, where n is the number of items in a list or set. 
   

One of the improvements would be to remove the in-order traverse. If the algorithm were to arrange alphabetical order at the same time when every character is being traversed (this alphabetical order would be reset word by word), the computer would not have to create the tree and run in-order traverse. If this idea realized, the time complexity would reduce to the O(n*179), where n is the number of characters in the test file. 

 

3. What is the time complexity of your code for part 2? How can you improve it? 

The time complexity is O((n-1)+n) => O(n), where n is the number of characters of a word (user inputs). Suppose that the pandas’ Dataframe is the same structure as the dictionary because the code “df.loc[index name]” can access its values. Once we can obtain the alphabetical order of the word’s characters, it enables users to access its value with constant time. 

The possible improvement is the same as in question 2.  My function “anagram_search” has two traversals of the word’s characters. Therefore, if the algorithm was able to create the alphabetical order within only one loop, the algorithm would be improved from O(2n-1) to O(n), even though the time complexity is still unchanged. 

 

4. Write a paragraph about what you have learnt in this project, and what you found most challenging. 

Through this project, I learned the importance of time complexity, more specifically how I should improve the data process speed rather than how accurate the codes and their outcomes are. Also, my most challenging part was to think how I should have reduced the complexity of the algorithm, in other words, time complexity as well. 

First of all, I was able to learn the importance of time complexity through the experience that I realized that Python packages didn’t execute specific processes with a constant time. For example, I used the regular expression operation (re module) to remove the punctuations and digits from the loaded text file. However, when I typed arguments of “re.sub()” method, I thought that this algorithm must traverse every character and specify whether the word is or not. I wouldn’t realize this perspective if I didn’t learn about time complexity. Furthermore, I used “stop_word()” method of another Python package (gensim) in order to remove the stop words at the beginning of starting this project. However, I searched for the source code of this method, then I found that this algorithm includes “split()" method and for loop to traverse items and remove only stop words. That is the reason why I didn’t employ those two methods in my anagram finder algorithm. Therefore, from this project, I could have improved my competence in checking the time complexity of every code for the algorithm to be more efficient. In other words, I was able to learn the importance of time complexity and run coding with what I learned. 

Also, the most challenging part of this project for me is to think about how the time complexity of my code can be improved more and more. I succeeded in reducing the time complexity of my algorithm by exploring the source code of Python packages. In addition, I could shorten the time complexity by using hashing strategy in the dictionary. More specifically, when the computer searches for whether a certain key exists in a dictionary or not, it is not necessary to traverse every dictionary key whose time complexity of the worst case is O(n), where n is the number of keys in a dictionary. All the computer has to do is whether a certain key has a corresponding value or not; when a value exists, a corresponding key must be in a dictionary. However, I think that there are more appropriate and efficient algorithms than what I coded. For example, my anagram searching function traverses every character of the input value twice, which is obviously inefficient, but the computer must do it in my algorithm. Therefore, I still think that my algorithm would not be fully efficient from the aspect of time complexity, and I feel that the exploration of how programmers should improve time complexity is challenging. 

All in all, time complexity is what I learned and what I felt was challenging in this project. I intend to create a lot of computer algorithms in the future, so I would like to focus on not only the accuracy of the outcome (whether the result is realizing what I expected) but also time complexity (whether the algorithm is appropriate about speed and simplicity) every time. 