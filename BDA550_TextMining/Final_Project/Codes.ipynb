{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BDM550 - Final Project\n",
    "\n",
    "- Name: Ran Arino\n",
    "- Student ID: 153073200\n",
    "- Email: rarino@myseneca.ca\n",
    "- Course: Predictive Analytics\n",
    "- Course ID: BDM550NAA.05359.2237\n",
    "- Professor: Dr. Elnaz Delpisheh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "- Run the Code A if you did not clean the texts\n",
    "- If you have already created \"clean_df.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# load the dataset\\na1 = pd.read_csv('articles1.csv')\\na2 = pd.read_csv('articles2.csv')\\na3 = pd.read_csv('articles3.csv')\\n\\n# Define function to load and clean text data\\n#  this is similar function to the one that I used in Workshop05\\ndef clean_texts(raw_texts):\\n    # define result\\n    result = []\\n\\n    # set of stopwords\\n    stop_words = set(stopwords.words('english'))\\n    # set porter stemmers\\n    porter = nltk.PorterStemmer()\\n\\n    # traversing all sentences\\n    for sent in raw_texts:\\n        # tokenize\\n        tokens = word_tokenize(sent)\\n        # defined cleaned sentence\\n        clean_sent = ''\\n        # cleaning each sentence\\n        for w in tokens:\\n            # if 'w' is one of punctuations, skip to the next word\\n            if w in string.punctuation:\\n                continue\\n            # if 'w' is one of stop words, skip to the next word\\n            if w.lower() in stop_words:\\n                continue\\n            # add stemmed word to clean_sent\\n            clean_sent += porter.stem(w.lower()) + ' '\\n\\n        # add clean_sent to result (make sure that the last item is always blank)\\n        result += [clean_sent[:-1]]\\n\\n    return result\\n\\n# cleaning the text & put everything into one variable\\ntexts = []\\n\\n# traversing all files\\nfor df in [a1, a2, a3]:\\n    texts += clean_texts(df['content'])\\n    \\n# store the cleaned data as new df\\ntitles = list(a1['title']) + list(a2['title']) + list(a3['title'])\\ndates = list(a1['date']) + list(a2['date']) + list(a3['date'])\\n\\nclean_df = pd.DataFrame({'title': titles, 'date': dates, 'content': texts})\\nclean_df.head()\\n\\n# download the clean_df\\n#clean_df.to_csv('clean_df', index=False)\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# load the dataset\n",
    "a1 = pd.read_csv('articles1.csv')\n",
    "a2 = pd.read_csv('articles2.csv')\n",
    "a3 = pd.read_csv('articles3.csv')\n",
    "\n",
    "# Define function to load and clean text data\n",
    "#  this is similar function to the one that I used in Workshop05\n",
    "def clean_texts(raw_texts):\n",
    "    # define result\n",
    "    result = []\n",
    "\n",
    "    # set of stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # set porter stemmers\n",
    "    porter = nltk.PorterStemmer()\n",
    "\n",
    "    # traversing all sentences\n",
    "    for sent in raw_texts:\n",
    "        # tokenize\n",
    "        tokens = word_tokenize(sent)\n",
    "        # defined cleaned sentence\n",
    "        clean_sent = ''\n",
    "        # cleaning each sentence\n",
    "        for w in tokens:\n",
    "            # if 'w' is one of punctuations, skip to the next word\n",
    "            if w in string.punctuation:\n",
    "                continue\n",
    "            # if 'w' is one of stop words, skip to the next word\n",
    "            if w.lower() in stop_words:\n",
    "                continue\n",
    "            # add stemmed word to clean_sent\n",
    "            clean_sent += porter.stem(w.lower()) + ' '\n",
    "\n",
    "        # add clean_sent to result (make sure that the last item is always blank)\n",
    "        result += [clean_sent[:-1]]\n",
    "\n",
    "    return result\n",
    "\n",
    "# cleaning the text & put everything into one variable\n",
    "texts = []\n",
    "\n",
    "# traversing all files\n",
    "for df in [a1, a2, a3]:\n",
    "    texts += clean_texts(df['content'])\n",
    "    \n",
    "# store the cleaned data as new df\n",
    "titles = list(a1['title']) + list(a2['title']) + list(a3['title'])\n",
    "dates = list(a1['date']) + list(a2['date']) + list(a3['date'])\n",
    "\n",
    "df_clean = pd.DataFrame({'title': titles, 'date': dates, 'content': texts})\n",
    "df_clean = clean_df.dropna(subset=['content'])\n",
    "df_clean.head()\n",
    "\n",
    "# download the clean_df\n",
    "#clean_df.to_csv('df_clean.csv', index=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>House Republicans Fret About Winning Their Hea...</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>washington — congression republican new fear c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rift Between Officers and Residents as Killing...</td>\n",
       "      <td>2017-06-19</td>\n",
       "      <td>bullet shell get count blood dri votiv candl b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tyrus Wong, ‘Bambi’ Artist Thwarted by Racial ...</td>\n",
       "      <td>2017-01-06</td>\n",
       "      <td>walt disney ’ “ bambi ” open 1942 critic prais...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Among Deaths in 2016, a Heavy Toll in Pop Musi...</td>\n",
       "      <td>2017-04-10</td>\n",
       "      <td>death may great equal ’ necessarili evenhand f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kim Jong-un Says North Korea Is Preparing to T...</td>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>seoul south korea — north korea ’ leader kim s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title        date  \\\n",
       "0  House Republicans Fret About Winning Their Hea...  2016-12-31   \n",
       "1  Rift Between Officers and Residents as Killing...  2017-06-19   \n",
       "2  Tyrus Wong, ‘Bambi’ Artist Thwarted by Racial ...  2017-01-06   \n",
       "3  Among Deaths in 2016, a Heavy Toll in Pop Musi...  2017-04-10   \n",
       "4  Kim Jong-un Says North Korea Is Preparing to T...  2017-01-02   \n",
       "\n",
       "                                             content  \n",
       "0  washington — congression republican new fear c...  \n",
       "1  bullet shell get count blood dri votiv candl b...  \n",
       "2  walt disney ’ “ bambi ” open 1942 critic prais...  \n",
       "3  death may great equal ’ necessarili evenhand f...  \n",
       "4  seoul south korea — north korea ’ leader kim s...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('df_clean.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(142535, 260539)\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# vectorizing the texts\n",
    "vect = CountVectorizer()\n",
    "vect_texts = vect.fit_transform(df['content'])\n",
    "\n",
    "print(vect_texts.shape)\n",
    "print(vect_texts[:5].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model (20 topics)\n",
    "lda = LatentDirichletAllocation(n_components=20)\n",
    "results = lda.fit_transform(vect_texts)\n",
    "\n",
    "# get topic-word distribution\n",
    "topic_word_dist = lda.components_\n",
    "# get document-word distribution\n",
    "doc_word_dist = lda.transform(vect_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
