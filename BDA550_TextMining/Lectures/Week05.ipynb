{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency\n",
    "- vocabulary = unique sets of words\n",
    "- vector; rows show focuments; columns represent vocaburaly\n",
    "- focusing on a specific document, try to search the frequency of words.\n",
    "- do not consider the grammer and relative position of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    stem = nltk.stem.SnowballStemmer('english')\n",
    "    text = text.lower()\n",
    "\n",
    "    for token in nltk.word_tokenize(text):\n",
    "        if token in string.punctuation: continue\n",
    "        yield stem.stem(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus 01:  defaultdict(<class 'int'>, {'the': 2, 'eleph': 1, 'sneez': 1, 'at': 1, 'sight': 1, 'of': 1, 'potato': 1})\n",
      "Corpus 02:  defaultdict(<class 'int'>, {'bat': 2, 'can': 1, 'see': 2, 'via': 1, 'echoloc': 1, 'the': 1, 'sight': 1, 'sneez': 1})\n",
      "Corpus 03:  defaultdict(<class 'int'>, {'wonder': 1, 'she': 1, 'opend': 1, 'the': 2, 'door': 1, 'to': 1, 'studio': 1})\n"
     ]
    }
   ],
   "source": [
    "corpus = ['The elephant sneezed at the sight of potatoes',\n",
    "          'Bats can see via echolocation. See the bat sight sneeze!',\n",
    "          'Wondering, she opend the door to the studio.']\n",
    "          \n",
    "tokenize(corpus)\n",
    "def freq_vectorize(doc):\n",
    "    features = defaultdict(int)\n",
    "    for token in tokenize(doc):\n",
    "        features[token] += 1\n",
    "    return features\n",
    "\n",
    "vectors = list(map(freq_vectorize, corpus))\n",
    "print(\"Corpus 01: \", vectors[0])\n",
    "print(\"Corpus 02: \", vectors[1])\n",
    "print(\"Corpus 03: \", vectors[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words list:\n",
      "['at' 'bat' 'bats' 'can' 'door' 'echolocation' 'elephant' 'of' 'opend'\n",
      " 'potatoes' 'see' 'she' 'sight' 'sneeze' 'sneezed' 'studio' 'the' 'to'\n",
      " 'via' 'wondering']\n",
      "\n",
      "Frequency Vectors:\n",
      "[[1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 2 0 0 0]\n",
      " [0 1 1 1 0 1 0 0 0 0 2 0 1 1 0 0 1 0 1 0]\n",
      " [0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 1 2 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectors = vectorizer.fit_transform(corpus)\n",
    "print('Words list:')\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print('')\n",
    "print('Frequency Vectors:')\n",
    "print(vectors.todense())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot Encoding\n",
    "- effective for very small documents that do not contain many repeated element.\n",
    "- do not care about the importance of words by frequency.\n",
    "- using for artificial neural networks, whose activation functions require input to be in the discrete ranges.\n",
    "\n",
    "- resprenting similality and difference at the document level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'the': True, 'eleph': True, 'sneez': True, 'at': True, 'sight': True, 'of': True, 'potato': True}, {'bat': True, 'can': True, 'see': True, 'via': True, 'echoloc': True, 'the': True, 'sight': True, 'sneez': True}, {'wonder': True, 'she': True, 'opend': True, 'the': True, 'door': True, 'to': True, 'studio': True}]\n"
     ]
    }
   ],
   "source": [
    "corpus  = [tokenize(doc) for doc in corpus]\n",
    "\n",
    "def onehot_vectorize(doc):\n",
    "    return {\n",
    "        token : True \n",
    "        for token in doc\n",
    "    }\n",
    "\n",
    "onehot_vectors = map(onehot_vectorize, corpus)\n",
    "print(list(onehot_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0],\n",
       "       [0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "corpus = ['The elephant sneezed at the sight of potatoes',\n",
    "          'Bats can see via echolocation. See the bat sight sneeze!',\n",
    "          'Wondering, she opend the door to the studio.']\n",
    "\n",
    "freq   = CountVectorizer()\n",
    "corpus = freq.fit_transform(corpus)\n",
    "\n",
    "onehot = Binarizer()\n",
    "corpus = onehot.fit_transform(corpus.toarray())\n",
    "\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency-Inverse Document Frequecy (TF-IDF)\n",
    "- Consider relative frequency or rareness of tokens in the document against their frequency in other docuements.\n",
    "- Rare words can give us the important insight.\n",
    "- Higher score to terms that are very relevant to a specific instance.\n",
    "- computing a per-term basis\n",
    "- measured by \n",
    "\n",
    "- TF (term frequency): number of times a term t occurs in document d.\n",
    "- IDF (inverse document frequency): log(number of documents / number of occurrences of term t in all documents.)\n",
    "\n",
    "- stop-words are weighted lower due to more frequent\n",
    "- widely using for bag-og-words models\n",
    "\n",
    "- why we use log?\n",
    "    - for the purpose of normalization\n",
    "    - The more words are appared in the whole documents, the lower the IDF is.\n",
    "      -> the larger the denominator, the lower the IDF is -> Not rare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object tfidf_vectorize at 0x000001E04C66EDC0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.text import TextCollection\n",
    "\n",
    "corpus = ['The elephant sneezed at the sight of potatoes',\n",
    "          'Bats can see via echolocation. See the bat sight sneeze!',\n",
    "          'Wondering, she opend the door to the studio.']\n",
    "\n",
    "def tfidf_vectorize(corpus):\n",
    "    corpus = [tokenize(doc) for doc in corpus]\n",
    "    texts = TextCollection(corpus)\n",
    "\n",
    "    for doc in corpus:\n",
    "        yield {\n",
    "            term: texts.tf_idf(term, doc)\n",
    "            for term in doc\n",
    "        }\n",
    "\n",
    "tfidf = tfidf_vectorize(corpus)\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words list: \n",
      "['at' 'bat' 'bats' 'can' 'door' 'echolocation' 'elephant' 'of' 'opend'\n",
      " 'potatoes' 'see' 'she' 'sight' 'sneeze' 'sneezed' 'studio' 'the' 'to'\n",
      " 'via' 'wondering']\n",
      " \n",
      "Vector\n",
      "[[0.37867627 0.         0.         0.         0.         0.\n",
      "  0.37867627 0.37867627 0.         0.37867627 0.         0.\n",
      "  0.28799306 0.         0.37867627 0.         0.44730461 0.\n",
      "  0.         0.        ]\n",
      " [0.         0.30251368 0.30251368 0.30251368 0.         0.30251368\n",
      "  0.         0.         0.         0.         0.60502736 0.\n",
      "  0.23006945 0.30251368 0.         0.         0.17866945 0.\n",
      "  0.30251368 0.        ]\n",
      " [0.         0.         0.         0.         0.36772387 0.\n",
      "  0.         0.         0.36772387 0.         0.         0.36772387\n",
      "  0.         0.         0.         0.36772387 0.43436728 0.36772387\n",
      "  0.         0.36772387]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = ['The elephant sneezed at the sight of potatoes',\n",
    "          'Bats can see via echolocation. See the bat sight sneeze!',\n",
    "          'Wondering, she opend the door to the studio.']\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "corpus = tfidf.fit_transform(corpus)\n",
    "\n",
    "print('Words list: ')\n",
    "print(tfidf.get_feature_names_out())\n",
    "print(' ')\n",
    "print('Vector')\n",
    "print(corpus.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disributed Representation\n",
    "\n",
    "- for Neural Networks\n",
    "- Context-based, continuous term similarity encoding\n",
    "- performance is intensive, so using the pre-trained model\n",
    "\n",
    "Word2vec\n",
    "- implementing a word ebedding model tha enables us to create these kinds of distributed representations.\n",
    "- training word reporesentations based on either a continueous bag-of-words(CBOW) or skip-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
