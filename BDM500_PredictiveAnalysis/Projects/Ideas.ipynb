{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you for sharing your idea. I will try to explain the math equations at each step and evaluate your method.\n",
    "\n",
    "First, let me define some notation and variables. Let X be the matrix of features, y be the vector of target values, theta be the vector of parameters, m be the number of training examples, n be the number of features, alpha be the learning rate, and J(theta) be the cost function. Assume that X has a column of ones for the intercept term, and that X and y are normalized and standardized.\n",
    "\n",
    "- Taking the best parameter at each step by normal equation; the training data is always the last 10 years data; once the newly updated data, the training data will be shifted.\n",
    "    - The normal equation is given by:\n",
    "\n",
    "    $$\\theta = (X^TX)^{-1}X^Ty$$\n",
    "\n",
    "    - This equation computes the optimal parameters that minimize the cost function J(theta), which is defined as:\n",
    "\n",
    "    $$J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "    - where $h_\\theta(x^{(i)})$ is the hypothesis function that predicts the value of y for a given x, and is defined as:\n",
    "\n",
    "    $$h_\\theta(x^{(i)}) = \\theta^Tx^{(i)}$$\n",
    "\n",
    "    - The normal equation requires to invert the matrix $X^TX$, which has a dimension of n x n. This can be computationally expensive and numerically unstable when n is large or when X is ill-conditioned. Therefore, this method may not be suitable for high-dimensional or noisy data.\n",
    "\n",
    "- Get the mean squared error, which is calculated by taking the sum of squared of the difference between the predicted values (from the current parameters and the last 10 years data) and the actual data.\n",
    "    - The mean squared error (MSE) is given by:\n",
    "\n",
    "    $$MSE = \\frac{1}{m}\\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "    - This is equivalent to J(theta) multiplied by 2/m. Therefore, minimizing MSE is equivalent to minimizing J(theta). However, MSE is sensitive to outliers and assumes homoscedasticity and normality of the error term. Therefore, this method may not be robust or valid for data that violates these assumptions.\n",
    "\n",
    "- Update the parameters for the next step by subtracting the mean squared error * learning rate from the current parameters.\n",
    "    - This update rule is given by:\n",
    "\n",
    "    $$\\theta := \\theta - \\alpha MSE$$\n",
    "\n",
    "    - This rule updates all parameters simultaneously by subtracting a scalar value from them. However, this rule does not take into account the gradient of J(theta) with respect to each parameter, which indicates how much each parameter contributes to the cost function and how much it should change to reduce it. Therefore, this rule may not guarantee the convergence or optimality of the parameters, and may even cause them to diverge or oscillate around the minimum.\n",
    "\n",
    "In summary, your idea has some drawbacks and limitations that may affect its performance and validity. A possible improvement is to use a gradient-based method instead of an error-based method, such as gradient descent or its variants. Gradient descent updates each parameter by taking steps in the opposite direction of its partial derivative with respect to J(theta), which is given by:\n",
    "\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m}\\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}$$\n",
    "\n",
    "The update rule for gradient descent is then:\n",
    "\n",
    "$$\\theta_j := \\theta_j - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_j}$$\n",
    "\n",
    "This rule ensures that each parameter moves towards the direction that decreases J(theta) most rapidly, and converges to a local minimum or a global minimum if J(theta) is convex. Gradient descent can also be modified to use different learning rates, momentum terms, or adaptive methods to improve its speed and stability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "<hr style=\"border:2px solid gray\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic regularization is a method of adding a penalty term to the loss function of a linear or logistic regression model, which helps to prevent overfitting and improve generalization. The penalty term is a linear combination of the L1-norm and the L2-norm of the model coefficients, which are also known as the elastic net coefficients. The L1-norm encourages sparsity, meaning that some coefficients can be shrunk to zero and eliminated from the model. The L2-norm encourages smoothness, meaning that the coefficients are kept small and close to each other.\n",
    "\n",
    "The elastic net loss function can be written as:\n",
    "\n",
    "$$\n",
    "L(\\beta) = \\sum_{i=1}^n (y_i - x_i^T \\beta)^2 + \\lambda_1 \\sum_{j=1}^p |\\beta_j| + \\lambda_2 \\sum_{j=1}^p \\beta_j^2\n",
    "$$\n",
    "\n",
    "where $\\beta$ is the vector of elastic net coefficients, $x_i$ is the vector of features for the $i$-th observation, $y_i$ is the response variable for the $i$-th observation, $n$ is the number of observations, $p$ is the number of features, $\\lambda_1$ and $\\lambda_2$ are the regularization parameters that control the strength of the L1 and L2 penalties, respectively.\n",
    "\n",
    "To find the optimal values of $\\beta$, we need to minimize the loss function with respect to $\\beta$. This can be done using gradient descent, which is an iterative algorithm that updates $\\beta$ by moving in the opposite direction of the gradient of the loss function. The gradient of the loss function is given by:\n",
    "\n",
    "$$\n",
    "\\nabla L(\\beta) = -2 \\sum_{i=1}^n x_i (y_i - x_i^T \\beta) + 2 \\lambda_2 \\beta + \\lambda_1 s\n",
    "$$\n",
    "\n",
    "where $s$ is a vector of signs of $\\beta$, such that $s_j = sign(\\beta_j)$ for $j = 1, ..., p$. The sign function is defined as:\n",
    "\n",
    "$$\n",
    "sign(x) = \\begin{cases}\n",
    "-1 & \\text{if } x < 0 \\\\\n",
    "0 & \\text{if } x = 0 \\\\\n",
    "1 & \\text{if } x > 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The gradient descent update rule for $\\beta$ is:\n",
    "\n",
    "$$\n",
    "\\beta^{(t+1)} = \\beta^{(t)} - \\alpha \\nabla L(\\beta^{(t)})\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is the learning rate, which determines how big of a step to take in each iteration, and $t$ is the iteration number.\n",
    "\n",
    "Now, suppose we have already found an optimal value of $\\beta$ using some existing data, and we want to update it by adding one new observation $(x_{n+1}, y_{n+1})$. How does elastic regularization affect this update?\n",
    "\n",
    "To answer this question, we need to compare the loss function before and after adding the new observation. The loss function before adding the new observation is:\n",
    "\n",
    "$$\n",
    "L(\\beta) = \\sum_{i=1}^n (y_i - x_i^T \\beta)^2 + \\lambda_1 \\sum_{j=1}^p |\\beta_j| + \\lambda_2 \\sum_{j=1}^p \\beta_j^2\n",
    "$$\n",
    "\n",
    "The loss function after adding the new observation is:\n",
    "\n",
    "$$\n",
    "L'(\\beta) = \\sum_{i=1}^{n+1} (y_i - x_i^T \\beta)^2 + \\lambda_1 \\sum_{j=1}^p |\\beta_j| + \\lambda_2 \\sum_{j=1}^p \\beta_j^2\n",
    "$$\n",
    "\n",
    "The difference between these two loss functions is:\n",
    "\n",
    "$$\n",
    "\\Delta L(\\beta) = L'(\\beta) - L(\\beta) = (y_{n+1} - x_{n+1}^T \\beta)^2\n",
    "$$\n",
    "\n",
    "This means that adding a new observation only affects the loss function by adding a squared error term. The regularization terms do not change because they do not depend on the data.\n",
    "\n",
    "Therefore, to update $\\beta$ by adding a new observation, we only need to apply one step of gradient descent using the new observation. The gradient of the loss function with respect to the new observation is:\n",
    "\n",
    "$$\n",
    "\\nabla L'(\\beta) = -2 x_{n+1} (y_{n+1} - x_{n+1}^T \\beta) + 2 \\lambda_2 \\beta + \\lambda_1 s\n",
    "$$\n",
    "\n",
    "The gradient descent update rule for $\\beta$ with respect to the new observation is:\n",
    "\n",
    "$$\n",
    "\\beta^{(t+1)} = \\beta^{(t)} - \\alpha \\nabla L'(\\beta^{(t)})\n",
    "$$\n",
    "\n",
    "This update rule shows that elastic regularization affects the update of $\\beta$ by adding two terms: $2 \\lambda_2 \\beta$ and $\\lambda_1 s$. The first term is the same as in ridge regression, which shrinks $\\beta$ towards zero by a factor of $2 \\lambda_2$. The second term is the same as in lasso regression, which shrinks $\\beta$ towards zero by a constant amount of $\\lambda_1$, but also changes the sign of $\\beta$ if it is close to zero. This term can cause some coefficients to become exactly zero and be eliminated from the model.\n",
    "\n",
    "In summary, elastic regularization affects the update of $\\beta$ by adding one new observation by applying one step of gradient descent using the new observation and the regularization terms. The regularization terms help to prevent overfitting and improve generalization by shrinking and sparsifying $\\beta$.\n",
    "\n",
    "Source: Conversation with Bing, 10/20/2023\n",
    "(1) Elastic net regularization - Wikipedia. https://en.wikipedia.org/wiki/Elastic_net_regularization.\n",
    "(2) Elastic Net Regression Explained, Step by Step - Machine Learning Compass. https://machinelearningcompass.com/machine_learning_models/elastic_net_regression/.\n",
    "(3) Elastic Net - Overview, Geometry, and Regularization. https://corporatefinanceinstitute.com/resources/data-science/elastic-net/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, you are right. If you only use the newly updated data to update your model, then you are using a stochastic gradient descent (SGD) approach. SGD is a variant of gradient descent that updates the parameters using only a single or a small batch of data at each iteration, instead of using the entire dataset. This can reduce the computational cost and memory requirement of the algorithm, and also introduce some randomness and noise that can help the model escape from local minima or saddle points.\n",
    "\n",
    "The calculations for SGD are similar to those for gradient descent, except that you use a different subset of data at each iteration. Let me show you an example with a single data point. Suppose you have a new data point $(x^{(i)}, y^{(i)})$ that arrives at time t, and you want to update your parameters $\\theta$ using SGD. You can do the following steps:\n",
    "\n",
    "- Calculate the hypothesis function $h_\\theta(x^{(i)}) = \\theta^Tx^{(i)}$, which predicts the value of y for the given x.\n",
    "- Calculate the error term $e^{(i)} = h_\\theta(x^{(i)}) - y^{(i)}$, which measures the difference between the predicted value and the actual value.\n",
    "- Calculate the gradient of the cost function $J(\\theta)$ with respect to each parameter $\\theta_j$, which is given by $\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = e^{(i)}x_j^{(i)}$. Note that this is different from the batch gradient descent, where you have to sum over all data points.\n",
    "- Update each parameter $\\theta_j$ by taking a step in the opposite direction of its gradient, which is given by $\\theta_j := \\theta_j - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\theta_j - \\alpha e^{(i)}x_j^{(i)}$. Note that this is similar to the batch gradient descent, except that you use a different error term and feature value at each iteration.\n",
    "\n",
    "You can repeat these steps for each new data point that arrives, and your model will gradually converge to a local minimum or a global minimum if $J(\\theta)$ is convex. You can also use a small batch of data instead of a single data point, and calculate the average gradient and error term over the batch.\n",
    "\n",
    "SGD has some advantages over batch gradient descent, such as:\n",
    "- It can handle large or streaming data more efficiently and effectively, since it does not require to store or process all the data at once.\n",
    "- It can adapt to changing data more quickly and smoothly, since it updates the parameters more frequently and incrementally.\n",
    "- It can avoid getting stuck in local minima or saddle points, since it introduces some randomness and noise that can help the model explore different regions of the cost function.\n",
    "\n",
    "However, SGD also has some drawbacks, such as:\n",
    "- It can be noisy and unstable, since it updates the parameters based on a single or a small batch of data that may not represent the true gradient or error term.\n",
    "- It can oscillate around the minimum or overshoot it, since it uses a fixed learning rate that may not be optimal for different stages of the convergence process.\n",
    "- It can be sensitive to outliers or noisy data, since it updates the parameters based on every data point regardless of its quality or relevance.\n",
    "\n",
    "Therefore, SGD requires careful tuning of the learning rate and other hyperparameters, such as batch size, momentum term, or adaptive methods, to improve its speed and stability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elastic net Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure, if the first column of the matrix **X_** is a bias term (all ones for instance), and the vector **theta_** includes the parameter **theta_0**, then the matrix multiplication will account for this bias term. \n",
    "\n",
    "Here's how it looks in matrix form:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & x_{12} & \\cdots & x_{1n} \\\\\n",
    "1 & x_{22} & \\cdots & x_{2n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & x_{m2} & \\cdots & x_{mn}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\theta_0 \\\\\n",
    "\\theta_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\theta_n\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\theta_0 + x_{12}\\theta_1 + \\cdots + x_{1n}\\theta_n \\\\\n",
    "\\theta_0 + x_{22}\\theta_1 + \\cdots + x_{2n}\\theta_n \\\\\n",
    "\\vdots \\\\\n",
    "\\theta_0 + x_{m2}\\theta_1 + \\cdots + x_{mn}\\theta_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Each element in the resulting vector is the sum of the products of corresponding elements in a row of **X** and the vector **θ**, including the bias term **θ0**. This is essentially a dot product operation performed for each row of **X** with **θ**, taking into account the bias. This operation is often used in machine learning algorithms such as linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
