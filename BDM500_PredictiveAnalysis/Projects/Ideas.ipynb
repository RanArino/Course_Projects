{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you for sharing your idea. I will try to explain the math equations at each step and evaluate your method.\n",
    "\n",
    "First, let me define some notation and variables. Let X be the matrix of features, y be the vector of target values, theta be the vector of parameters, m be the number of training examples, n be the number of features, alpha be the learning rate, and J(theta) be the cost function. Assume that X has a column of ones for the intercept term, and that X and y are normalized and standardized.\n",
    "\n",
    "- Taking the best parameter at each step by normal equation; the training data is always the last 10 years data; once the newly updated data, the training data will be shifted.\n",
    "    - The normal equation is given by:\n",
    "\n",
    "    $$\\theta = (X^TX)^{-1}X^Ty$$\n",
    "\n",
    "    - This equation computes the optimal parameters that minimize the cost function J(theta), which is defined as:\n",
    "\n",
    "    $$J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "    - where $h_\\theta(x^{(i)})$ is the hypothesis function that predicts the value of y for a given x, and is defined as:\n",
    "\n",
    "    $$h_\\theta(x^{(i)}) = \\theta^Tx^{(i)}$$\n",
    "\n",
    "    - The normal equation requires to invert the matrix $X^TX$, which has a dimension of n x n. This can be computationally expensive and numerically unstable when n is large or when X is ill-conditioned. Therefore, this method may not be suitable for high-dimensional or noisy data.\n",
    "\n",
    "- Get the mean squared error, which is calculated by taking the sum of squared of the difference between the predicted values (from the current parameters and the last 10 years data) and the actual data.\n",
    "    - The mean squared error (MSE) is given by:\n",
    "\n",
    "    $$MSE = \\frac{1}{m}\\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "    - This is equivalent to J(theta) multiplied by 2/m. Therefore, minimizing MSE is equivalent to minimizing J(theta). However, MSE is sensitive to outliers and assumes homoscedasticity and normality of the error term. Therefore, this method may not be robust or valid for data that violates these assumptions.\n",
    "\n",
    "- Update the parameters for the next step by subtracting the mean squared error * learning rate from the current parameters.\n",
    "    - This update rule is given by:\n",
    "\n",
    "    $$\\theta := \\theta - \\alpha MSE$$\n",
    "\n",
    "    - This rule updates all parameters simultaneously by subtracting a scalar value from them. However, this rule does not take into account the gradient of J(theta) with respect to each parameter, which indicates how much each parameter contributes to the cost function and how much it should change to reduce it. Therefore, this rule may not guarantee the convergence or optimality of the parameters, and may even cause them to diverge or oscillate around the minimum.\n",
    "\n",
    "In summary, your idea has some drawbacks and limitations that may affect its performance and validity. A possible improvement is to use a gradient-based method instead of an error-based method, such as gradient descent or its variants. Gradient descent updates each parameter by taking steps in the opposite direction of its partial derivative with respect to J(theta), which is given by:\n",
    "\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m}\\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}$$\n",
    "\n",
    "The update rule for gradient descent is then:\n",
    "\n",
    "$$\\theta_j := \\theta_j - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_j}$$\n",
    "\n",
    "This rule ensures that each parameter moves towards the direction that decreases J(theta) most rapidly, and converges to a local minimum or a global minimum if J(theta) is convex. Gradient descent can also be modified to use different learning rates, momentum terms, or adaptive methods to improve its speed and stability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "<hr style=\"border:2px solid gray\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, you are right. If you only use the newly updated data to update your model, then you are using a stochastic gradient descent (SGD) approach. SGD is a variant of gradient descent that updates the parameters using only a single or a small batch of data at each iteration, instead of using the entire dataset. This can reduce the computational cost and memory requirement of the algorithm, and also introduce some randomness and noise that can help the model escape from local minima or saddle points.\n",
    "\n",
    "The calculations for SGD are similar to those for gradient descent, except that you use a different subset of data at each iteration. Let me show you an example with a single data point. Suppose you have a new data point $(x^{(i)}, y^{(i)})$ that arrives at time t, and you want to update your parameters $\\theta$ using SGD. You can do the following steps:\n",
    "\n",
    "- Calculate the hypothesis function $h_\\theta(x^{(i)}) = \\theta^Tx^{(i)}$, which predicts the value of y for the given x.\n",
    "- Calculate the error term $e^{(i)} = h_\\theta(x^{(i)}) - y^{(i)}$, which measures the difference between the predicted value and the actual value.\n",
    "- Calculate the gradient of the cost function $J(\\theta)$ with respect to each parameter $\\theta_j$, which is given by $\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = e^{(i)}x_j^{(i)}$. Note that this is different from the batch gradient descent, where you have to sum over all data points.\n",
    "- Update each parameter $\\theta_j$ by taking a step in the opposite direction of its gradient, which is given by $\\theta_j := \\theta_j - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\theta_j - \\alpha e^{(i)}x_j^{(i)}$. Note that this is similar to the batch gradient descent, except that you use a different error term and feature value at each iteration.\n",
    "\n",
    "You can repeat these steps for each new data point that arrives, and your model will gradually converge to a local minimum or a global minimum if $J(\\theta)$ is convex. You can also use a small batch of data instead of a single data point, and calculate the average gradient and error term over the batch.\n",
    "\n",
    "SGD has some advantages over batch gradient descent, such as:\n",
    "- It can handle large or streaming data more efficiently and effectively, since it does not require to store or process all the data at once.\n",
    "- It can adapt to changing data more quickly and smoothly, since it updates the parameters more frequently and incrementally.\n",
    "- It can avoid getting stuck in local minima or saddle points, since it introduces some randomness and noise that can help the model explore different regions of the cost function.\n",
    "\n",
    "However, SGD also has some drawbacks, such as:\n",
    "- It can be noisy and unstable, since it updates the parameters based on a single or a small batch of data that may not represent the true gradient or error term.\n",
    "- It can oscillate around the minimum or overshoot it, since it uses a fixed learning rate that may not be optimal for different stages of the convergence process.\n",
    "- It can be sensitive to outliers or noisy data, since it updates the parameters based on every data point regardless of its quality or relevance.\n",
    "\n",
    "Therefore, SGD requires careful tuning of the learning rate and other hyperparameters, such as batch size, momentum term, or adaptive methods, to improve its speed and stability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elastic net Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Elastic Net regularization is a linear combination of L1 and L2 regularizations. It adds both L1 and L2 penalties to the loss function:\n",
    "\n",
    "$$ J(\\theta) = MSE(\\theta) + r \\cdot \\alpha \\cdot ||\\theta||_1 + \\frac{1-r}{2} \\cdot \\alpha \\cdot ||\\theta||_2^2 $$\n",
    "\n",
    "where:\n",
    "- $J(\\theta)$ is the cost function\n",
    "- $MSE(\\theta)$ is the Mean Squared Error\n",
    "- $r$ is the mixing parameter between Ridge ($r=0$) and Lasso ($r=1$)\n",
    "- $\\alpha$ is the regularization parameter\n",
    "- $||\\theta||_1$ is the L1 norm (sum of absolute values of the parameters)\n",
    "- $||\\theta||_2^2$ is the squared L2 norm (sum of squares of the parameters)\n",
    "\n",
    "The derivative of this cost function with respect to the parameters $\\theta$ is:\n",
    "\n",
    "$$ \\frac{d}{d\\theta} J(\\theta) = \\frac{d}{d\\theta} MSE(\\theta) + r \\cdot \\alpha \\cdot sign(\\theta) + (1-r) \\cdot \\alpha \\cdot \\theta $$\n",
    "\n",
    "where $sign(\\theta)$ is the sign function, which outputs -1 for negative inputs, 0 for zero, and 1 for positive inputs.\n",
    "\n",
    "In Python, you can compute this derivative as follows:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def elastic_net_derivative(theta, X, y, alpha, r):\n",
    "    n = len(X)\n",
    "    y_hat = np.dot(X, theta)\n",
    "    d_mse = 2/n * np.dot(X.T, y_hat - y)\n",
    "    d_l1 = r * alpha * np.sign(theta)\n",
    "    d_l2 = (1 - r) * alpha * theta\n",
    "    return d_mse + d_l1 + d_l2\n",
    "```\n",
    "\n",
    "In this code:\n",
    "- `X` is the matrix of input features,\n",
    "- `y` is the vector of target values,\n",
    "- `alpha` is the regularization parameter,\n",
    "- `r` is the mixing parameter,\n",
    "- `theta` are the parameters of your model. \n",
    "\n",
    "This function computes and returns the derivative of the Elastic Net cost function with respect to `theta`. The derivative will be used in gradient descent to update the parameters. Please note that this code does not handle the bias term separately; you may want to do so in your implementation. Also, remember to scale your features before applying Elastic Net."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
